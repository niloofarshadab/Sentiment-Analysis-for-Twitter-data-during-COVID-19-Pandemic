{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device not found!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertConfig\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import ujson as json\n",
    "import pandas\n",
    "import time\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using CUDA device.\" if USE_CUDA else \"CUDA device not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "df = pandas.read_csv(\"./data/large_sentiment.csv\", encoding=\"cp1252\", header=None)\n",
    "df = df.to_numpy()\n",
    "\n",
    "y = list(df[:,0])#[0:10000]\n",
    "x = list(df[:,5])#[0:10000]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#x = tokenizer(x, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "x = tokenizer.batch_encode_plus(x, return_tensors='pt', padding=True)['input_ids']\n",
    "\n",
    "l_dict = {0:0, 2:1, 4:2}\n",
    "y = torch.tensor([l_dict[e] for e in y])\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"File Reading Time:\",f\"{t1-t0:.2f}\")\n",
    "print(\"Tokenization Time:\",f\"{t2-t1:.2f}\")\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "n = len(dataset)\n",
    "train_data, val_data, test_data = random_split(dataset, [int(0.8*n), int(0.1*n), int(0.1*n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, final=False):\n",
    "    val_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for i, (x, y) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            loss, logits = model(x, labels=y)[:2]\n",
    "\n",
    "            y_true.extend(y.detach().cpu().numpy().tolist())\n",
    "            y_pred.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    accuracy = sum(np.array(y_true) == np.array(y_pred)) / len(y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    if(final):\n",
    "        print(\"Results\")\n",
    "        print(\"\\tAvg Loss:\", val_loss)\n",
    "        print(\"\\tAccuracy:\", accuracy)\n",
    "        print(\"\\tF1 Score:\", f1)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    else:\n",
    "        print(\"Validation\", \"Avg Loss:\", val_loss, \" - Accuracy:\", accuracy,\"\\n\\n\")\n",
    "        return val_loss, accuracy\n",
    "\n",
    "def train_classifier(model, train_loader, val_loader=None, epochs=1):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    #wandb.init(project=\"toxic-bert\")\n",
    "    #config = wandb.config\n",
    "    #wandb.watch(model)\n",
    "\n",
    "    val_losses, val_accuracy, losses, seconds = [], [], [], []\n",
    "    if (val_loader != None):\n",
    "      val_loss, val_acc = validate(model, val_loader)\n",
    "      val_losses.append(val_loss)\n",
    "      val_accuracy.append(val_acc)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        t1 = time.perf_counter()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            loss, logits = model(x, labels=y)[:2]\n",
    "            #wandb.log({\"loss\": loss})\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            t2 = time.perf_counter()\n",
    "            eta = (t2 - t1) * len(train_loader) / i + 1\n",
    "            print(f'\\repoch: {e+1}/{epochs} | ETA: {eta} | batch: {i+1}/{len(train_loader)} | loss: {loss.item():.4f}'), end=\"\")\n",
    "            \n",
    "            if((i+1) % 100_000 == 0):\n",
    "                if (val_loader != None):\n",
    "                    print()\n",
    "                    val_loss, val_acc = validate(model, val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy.append(val_acc)\n",
    "                #model.save_pretrained(path + \"/saves/sent_{}_{}.pt\".format(e, i))\n",
    "        print()\n",
    "        #model.save_pretrained(\"./saves/sent_{}.pt\".format(e))\n",
    "        t2 = time.perf_counter()\n",
    "        seconds.append(t2 - t1)\n",
    "        \n",
    "    return val_losses, val_accuracy, losses, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device)\n",
    "val_losses, val_accuracy, losses, seconds = train_classifier(model, train_loader, val_loader=val_loader, epochs=1)\n",
    "\n",
    "validate(model, val_loader, final=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"./data/sentiment_tweets3.csv\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
